{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: [9.206 5.806 9.795 7.452], 0.014636\n",
      "Sample 2: [9.123 5.265 8.88  5.74 ], 0.011781\n",
      "Sample 3: [0.661 2.222 3.34  7.025], 2.3e-05\n",
      "Sample 4: [2.022 2.641 2.202 2.139], 3.2e-05\n",
      "Sample 5: [0.501 2.135 3.23  1.475], 8.6e-05\n",
      "Sample 6: [0.363 1.749 2.184 1.032], 1.7e-05\n",
      "Sample 7: [5.306 5.682 0.108 5.365], 0.001569\n",
      "Sample 8: [0.608 8.135 1.662 2.234], 0.004223\n",
      "Sample 9: [5.049 3.675 4.728 4.297], 0.000879\n",
      "Sample 10: [7.569 8.265 5.136 5.122], 0.006847\n",
      "Sample 11: [0.16  6.248 5.637 4.814], 0.001882\n",
      "Sample 12: [3.649 6.6   2.837 8.771], 0.001704\n",
      "[10.0, 10.0, 10.0, 10.0] 0.021217\n",
      "[10.0, 10.0, 10.0, 0.0] 0.019906\n",
      "[10.0, 7.385, 10.0, 0.0] 0.017182\n",
      "[9.67, 5.472, 9.727, 7.148] 0.014982\n",
      "[8.734, 9.685, 8.692, 5.309] 0.014895\n",
      "[10.0, 4.863, 10.0, 0.0] 0.014875\n",
      "[10.0, 0.0, 10.0, 10.0] 0.014653\n",
      "[10.0, 0.0, 10.0, 4.441] 0.012072\n",
      "[10.0, 0.0, 10.0, 0.0] 0.011726\n",
      "[10.0, 10.0, 0.0, 0.0] 0.01108\n",
      "[0.0, 10.0, 10.0, 0.0] 0.009689\n",
      "[8.929, 3.32, 8.212, 0.417] 0.009147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"#Prints concentrations, predicted value, and actual value\\nfor key in mins_to_test.keys():\\n    print(f'{key}:')\\n    for index in range(samples_per_method):\\n        print(mins_to_test[key][index][0].round(3), mins_to_test[key][index][1].round(6), conductivity_func(mins_to_test[key][index][0].round(6)))\\n\\n\""
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.optimize import minimize, rosen\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, WhiteKernel as W, RBF, Matern, RationalQuadratic as RQ, ExpSineSquared as ESS, DotProduct as DP, Sum, Product\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#===========================================================================================================#\n",
    "#Obtaining initial values\n",
    "\n",
    "def conductivity_func(X, finding_min=False):\n",
    "    if finding_min==False:\n",
    "        return (rosen(X.T) * 1e-8).round(6)\n",
    "    return -(rosen(X.T) * 1e-8).round(6)    #If values are being minimized, the results are put as negative to find a minimum\n",
    "\n",
    "np.random.seed(10003)\n",
    "samples = 12\n",
    "concentrations = 4\n",
    "X = (np.random.random((samples, concentrations))*10).round(3)   #Initializes the initial set of concentrations\n",
    "y = conductivity_func(X).round(6)                               #Initializes the initial set of conductivities\n",
    "\n",
    "for i in range(len(y)):                                         #Prints all values\n",
    "    print(f'Sample {i+1}: {X[i]}, {y[i]}')\n",
    "\n",
    "#===========================================================================================================#\n",
    "#Declaring Machine Learning Model Used\n",
    "\n",
    "#kernel = Matern(length_scale=1e+04, nu=1.5, length_scale_bounds=(100.0, 10000.0)) * DP(sigma_0=1e+04, sigma_0_bounds=(1.0, 100000.0))\n",
    "#GPR = GaussianProcessRegressor(kernel = kernel, alpha=1e-3, normalize_y=True)\n",
    "kernel = 31.6**2 * RBF(length_scale=10, length_scale_bounds=(10.0, 10000.0)) + DP(sigma_0=1e+04, sigma_0_bounds=(0.001, 10000.0))   #Best parameters from initial testing\n",
    "GPR = GaussianProcessRegressor(kernel = kernel, normalize_y=True, alpha=1e-8)                                                       #Forms ML model from said parameters\n",
    "\n",
    "\n",
    "#===========================================================================================================#\n",
    "#Bayesian Optimization\n",
    "\n",
    "def guess_in_bounds(bounds):                                                #This function produces guess values within a specfied bound\n",
    "    return [(b[1] - b[0]) * np.random.random() + b[0] for b in bounds]\n",
    "\n",
    "#The following functions are different acquisition functions to be used for Bayesian Optimization\n",
    "\n",
    "def upper_conf_bound(x, model, explore_weight, *args):          #Upper Confidence Bound Acquisition Function\n",
    "    pred, std = model.predict([x], return_std=True)\n",
    "    return -(pred+std*explore_weight)\n",
    "\n",
    "def probability_improve(x, model, best_y, *args):             #Probability of Improvement\n",
    "    pred, std = model.predict([x], return_std=True)\n",
    "    return -(norm.cdf((pred-best_y)/std))\n",
    "\n",
    "def expected_improvement(x, model, best_y, xi, *args):           #Expected Improvement Acquisition Function\n",
    "    pred, std = model.predict([x], return_std=True)\n",
    "    return -(pred-best_y-xi)*(norm.cdf((pred-best_y-xi)/std)+norm.pdf((pred-best_y-xi)/std))\n",
    "\n",
    "bounds = [(0., 10.)] * concentrations\n",
    "\n",
    "#===========================================================================================================#\n",
    "#Global Minimum\n",
    "\n",
    "samples_per_method = 6\n",
    "ucb_exploration_weight = 5\n",
    "xi = 0.5\n",
    "\n",
    "def in_list(arr, lst): #This function compares an array with a list of arrays to see if the array is already present\n",
    "    if lst == []:\n",
    "        return False\n",
    "    for item in lst:\n",
    "        if np.allclose(arr, item, atol=1e-3):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def ML_func(guess, model):\n",
    "    return -model.predict(guess.reshape(1, -1))\n",
    "\n",
    "def save_points(old_mins, new_mins):\n",
    "    old_x = [mins[0] for mins in old_mins]\n",
    "    new_x = [mins[0] for mins in new_mins]\n",
    "    points_added = 0\n",
    "    index = 0\n",
    "    while points_added < samples_per_method:\n",
    "        if in_list(new_x[index], old_x) == False:\n",
    "            old_mins.append((new_mins[index][0].tolist(), new_mins[index][1]))\n",
    "            points_added += 1\n",
    "        index += 1\n",
    "    return old_mins\n",
    "\n",
    "def new_experiments1(concentrations, conductivities):\n",
    "    bayes_model = clone(GPR).fit(concentrations, conductivities)\n",
    "    model = clone(GPR).fit(concentrations, conductivities)\n",
    "    func_mins = []\n",
    "    X_copy = [val for val in X]\n",
    "    y_copy = [val for val in y]\n",
    "    iterations = 20\n",
    "    \n",
    "    #==========================================================================================#\n",
    "    #Non-Bayesian Optimization\n",
    "    current_iteration = 0\n",
    "    while (current_iteration<iterations):\n",
    "        np.random.seed(current_iteration)\n",
    "        guess = guess_in_bounds(bounds)\n",
    "        func_min = minimize(ML_func, guess, bounds=bounds, method='Nelder-Mead', args=(model))\n",
    "        x_guess = func_min.x.round(3)\n",
    "        y_guess = func_min.fun.round(6)\n",
    "        if in_list(x_guess, X_copy) == False:\n",
    "            X_copy.append(x_guess)\n",
    "            y_copy.append(y_guess)\n",
    "            func_mins.append((x_guess, -y_guess))\n",
    "        current_iteration += 1\n",
    "    \n",
    "    #==========================================================================================#\n",
    "    #Bayesian Optimization with Upper Bound Confidence\n",
    "    current_iteration = 0\n",
    "    while (current_iteration<iterations) or (len(func_mins)<samples_per_method):\n",
    "        np.random.seed(current_iteration)\n",
    "        bayes_model.fit(X_copy, y_copy)\n",
    "        guess = guess_in_bounds(bounds)\n",
    "        func_min = minimize(upper_conf_bound, guess, bounds=bounds, method='Nelder-Mead', args=(bayes_model, ucb_exploration_weight))\n",
    "        x_guess = func_min.x.round(3)\n",
    "        y_guess = model.predict(x_guess.reshape(1, -1)).round(6)\n",
    "        if in_list(x_guess, X_copy) == False:\n",
    "            X_copy.append(x_guess)\n",
    "            y_copy.append(float(y_guess))\n",
    "            func_mins.append((x_guess, y_guess.item()))\n",
    "        current_iteration += 1\n",
    "    '''x_axis = np.arange(1, len(func_mins)+1)\n",
    "    y_axis = [x[1] for x in func_mins]\n",
    "    plt.plot(x_axis, y_axis)'''\n",
    "    return sorted(func_mins, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def new_experiments2(concentrations, conductivities):\n",
    "    bayes_model = clone(GPR).fit(concentrations, conductivities)\n",
    "    model = clone(GPR).fit(concentrations, conductivities)\n",
    "    func_mins = []\n",
    "    X_copy = [val for val in X]\n",
    "    y_copy = [val for val in y]\n",
    "    iterations = 10\n",
    "    \n",
    "    #==========================================================================================#\n",
    "    #Bayesian Optimization with Probability of Improvement\n",
    "    current_iteration = 0\n",
    "    while (current_iteration<iterations):\n",
    "        np.random.seed(current_iteration)\n",
    "        bayes_model.fit(X_copy, y_copy)\n",
    "        guess = guess_in_bounds(bounds)\n",
    "        func_min = minimize(probability_improve, guess, bounds=bounds, method='Nelder-Mead', args=(bayes_model, max(y_copy)))\n",
    "        x_guess = func_min.x.round(3)\n",
    "        y_guess = model.predict(x_guess.reshape(1, -1)).round(6)\n",
    "        if in_list(x_guess, X_copy) == False:\n",
    "            X_copy.append(x_guess)\n",
    "            y_copy.append(float(y_guess))\n",
    "            func_mins.append((x_guess, y_guess.item()))\n",
    "        current_iteration += 1\n",
    "    #==========================================================================================#\n",
    "    #Bayesian Optimization with Expected Improvement\n",
    "    current_iteration = 0\n",
    "    while (current_iteration<iterations) or (len(func_mins)<samples_per_method):\n",
    "        np.random.seed(current_iteration)\n",
    "        bayes_model.fit(X_copy, y_copy)\n",
    "        guess = guess_in_bounds(bounds)\n",
    "        func_min = minimize(expected_improvement, guess, bounds=bounds, method='Nelder-Mead', args=(bayes_model, max(y_copy), xi))\n",
    "        x_guess = func_min.x.round(3)\n",
    "        y_guess = model.predict(x_guess.reshape(1, -1)).round(6)\n",
    "        if in_list(x_guess, X_copy) == False:\n",
    "            X_copy.append(x_guess)\n",
    "            y_copy.append(float(y_guess))\n",
    "            func_mins.append((x_guess, y_guess.item()))\n",
    "        current_iteration += 1\n",
    "    '''x_axis = np.arange(1, len(func_mins)+1)\n",
    "    y_axis = [x[1] for x in func_mins]\n",
    "    plt.plot(x_axis, y_axis)'''\n",
    "    return sorted(func_mins, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "new_points_1 = new_experiments1(X, y)\n",
    "new_points_2 = new_experiments2(X, y)\n",
    "\n",
    "new_points = []\n",
    "save_points(new_points, new_points_1) \n",
    "save_points(new_points, new_points_2) \n",
    "\n",
    "new_points = sorted(new_points, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for point in new_points:\n",
    "    print(point[0], point[1])\n",
    "    \n",
    "\n",
    "'''#Prints concentrations, predicted value, and actual value\n",
    "for key in mins_to_test.keys():\n",
    "    print(f'{key}:')\n",
    "    for index in range(samples_per_method):\n",
    "        print(mins_to_test[key][index][0].round(3), mins_to_test[key][index][1].round(6), conductivity_func(mins_to_test[key][index][0].round(6)))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Finish function to save all x's and y's to single variable\n",
    "2. Implement system so minimums not repeated?\n",
    "3. Clean up variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1e-10, 'kernel': DotProduct(sigma_0=1) + 1**2 * RBF(length_scale=1)}\n",
      "-5.535074378550914e-06\n",
      "-0.003259498412910748\n",
      "[1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
      "{'alpha': 1e-10, 'kernel__k1__sigma_0': 100000, 'kernel__k2__k1__constant_value': 100000, 'kernel__k2__k2__length_scale': 1}\n",
      "-5.2763984956988345e-06\n",
      "-0.0012360976115956808\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.optimize import minimize, rosen\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, WhiteKernel as W, RBF, Matern, RationalQuadratic as RQ, ExpSineSquared as ESS, DotProduct as DP, Sum, Product\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#===========================================================================================================#\n",
    "#Obtaining initial values\n",
    "\n",
    "def conductivity_func(X, finding_min=False):\n",
    "    if finding_min==False:\n",
    "        return (rosen(X.T) * 1e-8).round(6)\n",
    "    elif finding_min==True:\n",
    "        return -(rosen(X.T) * 1e-8).round(6)\n",
    "\n",
    "np.random.seed(10003)\n",
    "samples = 12\n",
    "concentrations = 4\n",
    "X = (np.random.random((samples, concentrations))*10).round(3)\n",
    "y = conductivity_func(X).round(6)\n",
    "\n",
    "'''for i in range(len(y)):\n",
    "    print(f'Sample {i+1}: {X[i]}, {y[i]}')'''\n",
    "\n",
    "#===========================================================================================================#\n",
    "#Declaring Machine Learning Model Used\n",
    "\n",
    "model = GaussianProcessRegressor(normalize_y=True)\n",
    "\n",
    "def range_append(lower, upper):\n",
    "    lst = []\n",
    "    for exp in range(lower, upper):\n",
    "        lst.append(10**exp)\n",
    "    return lst\n",
    "\n",
    "alpha = range_append(-10, -6)\n",
    "\n",
    "kernel = [C(), W(), RBF(), Matern(), RQ(), ESS(), DP()]\n",
    "operation = [Sum, Product]\n",
    "\n",
    "operation_list = []\n",
    "for op1 in operation:\n",
    "    for op2 in operation:\n",
    "        for kern1 in kernel:\n",
    "            for kern2 in kernel:\n",
    "                for kern3 in kernel:\n",
    "                    operation_list.append(op1(kern1, op2(kern2, kern3)))\n",
    "                    \n",
    "#===========================================================================================================#\n",
    "\n",
    "param_grid = [\n",
    "    {'kernel': operation_list,\n",
    "     'alpha': alpha}\n",
    "]\n",
    "\n",
    "param_search1 = GridSearchCV(model, param_grid, n_jobs=-1, cv=11, scoring='neg_mean_squared_error',)\n",
    "param_search1.fit(X, y)\n",
    "\n",
    "best_param1 = param_search1.best_params_\n",
    "best_score1 = param_search1.best_score_\n",
    "\n",
    "#model = GaussianProcessRegressor(alpha=0, normalize_y=True)\n",
    "print(best_param1)\n",
    "print(best_score1)\n",
    "print(cross_val_score(model, X, y, cv=11, scoring='neg_mean_absolute_error').mean())\n",
    "\n",
    "#===========================================================================================================#\n",
    "\n",
    "sigma_0 = range_append(-6, 6)\n",
    "constant_value = range_append(-6, 6)\n",
    "length_scale = range_append(-6, 6)\n",
    "\n",
    "param_dist = {'kernel__k1__sigma_0': sigma_0,\n",
    "              'kernel__k2__k1__constant_value': constant_value,\n",
    "              'kernel__k2__k2__length_scale': length_scale,\n",
    "              'alpha': alpha}\n",
    "\n",
    "model = GaussianProcessRegressor(normalize_y=True, **best_param1)\n",
    "\n",
    "param_search2 = GridSearchCV(model, param_dist, n_jobs=-1, cv=11, scoring='neg_mean_squared_error')\n",
    "param_search2.fit(X, y)\n",
    "\n",
    "best_param2 = param_search2.best_params_\n",
    "best_score2 = param_search2.best_score_\n",
    "\n",
    "print(best_param2)\n",
    "print(best_score2)\n",
    "print(cross_val_score(model, X, y, cv=11, scoring='neg_mean_absolute_error').mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 1**2 * Matern(length_scale=1, nu=1.5) + RBF(length_scale=1), 'alpha': 1e-07}\n",
      "-7.1520469908419184e-06\n",
      "-0.003259498412910748\n",
      "{'kernel__k2__k2__length_scale': 100, 'kernel__k2__k1__length_scale': 1e-05, 'kernel__k1__constant_value': 100, 'alpha': 1e-10}\n",
      "-1.2732900242723579e-05\n",
      "-0.001503022509695826\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.optimize import minimize, rosen\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, WhiteKernel as W, RBF, Matern, RationalQuadratic as RQ, ExpSineSquared as ESS, DotProduct as DP, Sum, Product\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#===========================================================================================================#\n",
    "#Obtaining initial values\n",
    "\n",
    "def conductivity_func(X, finding_min=False):\n",
    "    if finding_min==False:\n",
    "        return (rosen(X.T) * 1e-8).round(6)\n",
    "    elif finding_min==True:\n",
    "        return -(rosen(X.T) * 1e-8).round(6)\n",
    "\n",
    "np.random.seed(10003)\n",
    "samples = 12\n",
    "concentrations = 4\n",
    "X = (np.random.random((samples, concentrations))*10).round(3)\n",
    "y = conductivity_func(X).round(6)\n",
    "\n",
    "'''for i in range(len(y)):\n",
    "    print(f'Sample {i+1}: {X[i]}, {y[i]}')'''\n",
    "\n",
    "#===========================================================================================================#\n",
    "#Declaring Machine Learning Model Used\n",
    "\n",
    "model = GaussianProcessRegressor(normalize_y=True)\n",
    "\n",
    "def range_append(lower, upper):\n",
    "    lst = []\n",
    "    for exp in range(lower, upper):\n",
    "        lst.append(10**exp)\n",
    "    return lst\n",
    "\n",
    "alpha = range_append(-10, -6)\n",
    "\n",
    "kernel = [C(), W(), RBF(), Matern(), RQ(), ESS(), DP()]\n",
    "operation = [Sum, Product]\n",
    "\n",
    "operation_list = []\n",
    "for op1 in operation:\n",
    "    for op2 in operation:\n",
    "        for kern1 in kernel:\n",
    "            for kern2 in kernel:\n",
    "                for kern3 in kernel:\n",
    "                    operation_list.append(op1(kern1, op2(kern2, kern3)))\n",
    "                    \n",
    "#===========================================================================================================#\n",
    "\n",
    "param_grid = [\n",
    "    {'kernel': operation_list,\n",
    "     'alpha': alpha}\n",
    "]\n",
    "\n",
    "param_search1 = RandomizedSearchCV(model, param_grid, n_jobs=-1, cv=11, scoring='neg_mean_squared_error',)\n",
    "param_search1.fit(X, y)\n",
    "\n",
    "best_param1 = param_search1.best_params_\n",
    "best_score1 = param_search1.best_score_\n",
    "\n",
    "#model = GaussianProcessRegressor(alpha=0, normalize_y=True)\n",
    "print(best_param1)\n",
    "print(best_score1)\n",
    "print(cross_val_score(model, X, y, cv=11, scoring='neg_mean_absolute_error').mean())\n",
    "\n",
    "#===========================================================================================================#\n",
    "\n",
    "sigma_0 = range_append(-5, 6)\n",
    "constant_value = range_append(-5, 6)\n",
    "length_scale = range_append(-5, 6)\n",
    "\n",
    "#print(sigma_0)\n",
    "\n",
    "param_dist = {'kernel__k1__constant_value': constant_value,\n",
    "              'kernel__k2__k1__length_scale': sigma_0,\n",
    "              'kernel__k2__k2__length_scale': length_scale,\n",
    "              'alpha': alpha}\n",
    "\n",
    "model = GaussianProcessRegressor(normalize_y=True, **best_param1)\n",
    "\n",
    "param_search2 = RandomizedSearchCV(model, param_dist, n_jobs=-1, cv=11, scoring='neg_mean_squared_error')\n",
    "param_search2.fit(X, y)\n",
    "\n",
    "best_param2 = param_search2.best_params_\n",
    "best_score2 = param_search2.best_score_\n",
    "\n",
    "print(best_param2)\n",
    "print(best_score2)\n",
    "print(cross_val_score(model, X, y, cv=11, scoring='neg_mean_absolute_error').mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel = 31.6**2 * RBF(length_scale=10, length_scale_bounds=(10.0, 10000.0)) + DP(sigma_0=1e+04, sigma_0_bounds=(0.001, 10000.0))\n",
    "\n",
    "model = GaussianProcessRegressor(kernel = kernel, normalize_y=True, alpha=1e-8)\n",
    "\n",
    "score = -5.251450383038068e-06\n",
    "\n",
    "error: -0.0012242895406499746\n",
    "\n",
    "\n",
    "{'kernel': Matern(length_scale=1e+04, nu=1.5) * DotProduct(sigma_0=1e+04), 'kernel__k1__length_scale_bounds': (100.0, 10000.0), 'kernel__k2__sigma_0_bounds': (1.0, 100000.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.206 5.806 9.795 7.452]\n",
      " [9.123 5.265 8.88  5.74 ]\n",
      " [0.661 2.222 3.34  7.025]\n",
      " [2.022 2.641 2.202 2.139]\n",
      " [0.501 2.135 3.23  1.475]\n",
      " [0.363 1.749 2.184 1.032]\n",
      " [5.306 5.682 0.108 5.365]\n",
      " [0.608 8.135 1.662 2.234]\n",
      " [5.049 3.675 4.728 4.297]\n",
      " [7.569 8.265 5.136 5.122]\n",
      " [0.16  6.248 5.637 4.814]\n",
      " [3.649 6.6   2.837 8.771]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 11 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n11 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\Coding\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\Coding\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n    estimator._validate_params()\n  File \"d:\\Coding\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n    validate_parameter_constraints(\n  File \"d:\\Coding\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'kernel' parameter of GaussianProcessRegressor must be None or an instance of 'sklearn.gaussian_process.kernels.Kernel'. Got [1**2, WhiteKernel(noise_level=1), RBF(length_scale=1), Matern(length_scale=1, nu=1.5), RationalQuadratic(alpha=1, length_scale=1), ExpSineSquared(length_scale=1, periodicity=1), DotProduct(sigma_0=1)] instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m GPR2 \u001b[38;5;241m=\u001b[39m GaussianProcessRegressor(kernel \u001b[38;5;241m=\u001b[39m kernel, normalize_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m)\n\u001b[0;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGPR1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneg_mean_squared_error\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean())\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(cross_val_score(GPR1, X, y, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmean())\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(cross_val_score(GPR2, X, y, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[1;32md:\\Coding\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    560\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 562\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\Coding\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32md:\\Coding\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:328\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    308\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m    309\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    310\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    311\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    326\u001b[0m )\n\u001b[1;32m--> 328\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "File \u001b[1;32md:\\Coding\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:414\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    408\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m     )\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 11 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n11 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\Coding\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\Coding\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n    estimator._validate_params()\n  File \"d:\\Coding\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n    validate_parameter_constraints(\n  File \"d:\\Coding\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'kernel' parameter of GaussianProcessRegressor must be None or an instance of 'sklearn.gaussian_process.kernels.Kernel'. Got [1**2, WhiteKernel(noise_level=1), RBF(length_scale=1), Matern(length_scale=1, nu=1.5), RationalQuadratic(alpha=1, length_scale=1), ExpSineSquared(length_scale=1, periodicity=1), DotProduct(sigma_0=1)] instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.optimize import minimize, rosen\n",
    "#from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, WhiteKernel as W, RBF, Matern, RationalQuadratic as RQ, ExpSineSquared as ESS, DotProduct as DP, Sum, Product\n",
    "\n",
    "def conductivity_func(X, finding_min=False):\n",
    "    if finding_min==False:\n",
    "        return (rosen(X.T) * 1e-8).round(6)\n",
    "    elif finding_min==True:\n",
    "        return -(rosen(X.T) * 1e-8).round(6)\n",
    "\n",
    "\n",
    "np.random.seed(10003)\n",
    "\n",
    "X = (np.random.random((12, 4))*10).round(3)\n",
    "y = conductivity_func(X).round(6)\n",
    "\n",
    "print(X)\n",
    "\n",
    "kernel1 = Matern(length_scale=1e+04, nu=1.5, length_scale_bounds=(100.0, 10000.0)) * DP(sigma_0=1e+04, sigma_0_bounds=(1.0, 100000.0))\n",
    "GPR1 = GaussianProcessRegressor(kernel = kernel)\n",
    "kernel2 = 31.6**2 * RBF(length_scale=10, length_scale_bounds=(10.0, 10000.0)) + DP(sigma_0=1e+04, sigma_0_bounds=(0.001, 10000.0))\n",
    "GPR2 = GaussianProcessRegressor(kernel = kernel, normalize_y=True, alpha=1e-8)\n",
    "model = RandomForestRegressor(n_estimators=8)\n",
    "\n",
    "print(cross_val_score(GPR1, X, y, cv=11, scoring='neg_mean_squared_error').mean())\n",
    "print(cross_val_score(GPR1, X, y, cv=11, scoring='neg_mean_absolute_error').mean())\n",
    "print(cross_val_score(GPR2, X, y, cv=11, scoring='neg_mean_squared_error').mean())\n",
    "print(cross_val_score(GPR2, X, y, cv=11, scoring='neg_mean_absolute_error').mean())\n",
    "print(cross_val_score(model, X, y, cv=11, scoring='neg_mean_squared_error').mean())\n",
    "print(cross_val_score(model, X, y, cv=11, scoring='neg_mean_absolute_error').mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
